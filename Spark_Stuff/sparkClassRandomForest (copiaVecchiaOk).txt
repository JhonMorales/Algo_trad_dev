import org.apache.spark.SparkContext
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.tree.configuration.Algo._
import org.apache.spark.mllib.tree.impurity.Gini
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.tree.RandomForest
import java.io.FileOutputStream
import java.io.ObjectOutputStream 

// Load and parse the data file.

/* My function    */

def classProbabilities( data: RDD[LabeledPoint]) : Array[ Double ] = 
{
	val countsByCategory = data.map( _.label).countByValue()
	val counts = countsByCategory.toArray.sortBy( _._1 ).map( _._2 )
	counts.map(_.toDouble / counts.sum )
}
/****************************/



//val trainingData = parsedData
//val testData = tdata.map { line =>
//  val parts = line.split(',').map(_.toDouble)
//  LabeledPoint(parts(0), Vectors.dense(parts.tail))
//}

//  Empty categoricalFeaturesInfo indicates all features are continuous.
val numClasses = 3
val categoricalFeaturesInfo = Map[Int, Int]( 0-> 24, 1 -> 4, 2 -> 2, 3 -> 2, 4 -> 2, 5-> 2, 6->2 )
val featureSubsetStrategy = "auto" // Let the algorithm choose.
val maxBins = 2048
var index = 0;
scala.tools.nsc.io.File("Risultati_USD_JPY_DELETE").appendAll("maxDepth" + "," + "numTrees" + "," + "testPer" + "," + "sellSignalsButBuy" + "," +
"sellSignalsOk"  + "," + "sellSignalButOut" + "," +
"BuySignalsButSell" + "," +
"BuySignalsButOut" + "," +
"BuySignalsOk" + "," +
"outSignalsOk"  + "," +
"dico_1" + ","+
"tot_1" + ","+
"dico_0" + ","+
"tot_0 "+ ","+
"dico_2" + ","+
"tot_2" + ","+
"\n")

for(   maxDepth <- Array( 25);
       cutOff <- Array(40, 40, 40);
       numTrees <- Array( 300);	
       impurity <- Array("entropy", "gini"))
yield{
val data_ = sc.textFile("hdfs:///user/hdfs/config/USDJPY-2015-01.csv_30_model")
val tdata = sc.textFile("hdfs:///user/hdfs/config/USDJPY-2015-02.csv_30_model")
val marzo = sc.textFile("hdfs:///user/hdfs/config/USDJPY-2015-03.csv_30_model")
val luglio= sc.textFile("hdfs:///user/hdfs/config/USDJPY-2015-07.csv_30_model")
val aprile = sc.textFile("hdfs:///user/hdfs/config/USDJPY-2015-04.csv_30_model")
val maggio = sc.textFile("hdfs:///user/hdfs/config/USDJPY-2015-05.csv_30_model")
val giugno = sc.textFile("hdfs:///user/hdfs/config/USDJPY-2015-06.csv_30_model")
val data = data_.union(tdata).union(marzo).union(aprile).union(maggio).union(giugno).union(luglio)

val parsedData = data.map { line =>
  val parts = line.split(',').map(_.toDouble)
  LabeledPoint(parts(0), Vectors.dense(parts.tail))
}
// Split the data into training and test sets (25% held out for testing)
val splits = parsedData.randomSplit(Array(0.75, 0.25))
val (trainingDataTmp, testData) = (splits(0), splits(1))
val trainingData = trainingDataTmp.map { line =>
          val r = scala.util.Random
          val v = r.nextInt(100)
          if( v < cutOff && v > 0 && line.label == 0) LabeledPoint(5, line.features )
          else line  
}.filter( x => x.label != 5 )
trainingData.cache 
testData.cache  
val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)

// Evaluate model on test instances and compute test error
val labelAndPreds = testData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
val testPer = labelAndPreds.filter(r => r._1 == r._2).count.toDouble / testData.count()

//println("Learned classification tree model:\n" + model.toDebugString)

val tot_1 = labelAndPreds.filter(r => r._1 == 1).count.toDouble
val tot_2 = labelAndPreds.filter(r => r._1 == 2).count.toDouble
val tot_0 = labelAndPreds.filter(r => r._1 == 0).count.toDouble
val ok_1 = labelAndPreds.filter(r => r._1 == 1 && r._2 == 1).count.toDouble
val dico_1 = labelAndPreds.filter(r => r._2 == 1).count.toDouble

ok_1/dico_1

val ok_2 = labelAndPreds.filter(r => r._1 == 2 && r._2 == 2).count.toDouble
val dico_2 = labelAndPreds.filter(r => r._2 == 2).count.toDouble

ok_2/dico_2

val ok_0 = labelAndPreds.filter(r => r._1 == 0 && r._2 == 0).count.toDouble
val dico_0 = labelAndPreds.filter(r => r._2 == 0).count.toDouble

ok_0/dico_0

println("Test performance Totale = " + testPer)
val sellSignalsButBuy = labelAndPreds.filter(r => r._1 == 1 && r._2 == 2).count.toDouble/dico_2
val sellSignalsOk = labelAndPreds.filter(r => r._1 == 2 && r._2 == 2).count.toDouble/dico_2
val sellSignalButOut = labelAndPreds.filter(r => r._1 == 0 && r._2 == 2).count.toDouble/dico_2

val BuySignalsButSell = labelAndPreds.filter(r => r._1 == 2 && r._2 == 1).count.toDouble/dico_1
val BuySignalsButOut = labelAndPreds.filter(r => r._1 == 0 && r._2 == 1).count.toDouble/dico_1
val BuySignalsOk = labelAndPreds.filter(r => r._1 == 1 && r._2 == 1).count.toDouble/dico_1

val outSignalsOk = labelAndPreds.filter(r => r._1 == 0 && r._2 == 0).count.toDouble/dico_0
val trainPriorProbabilities = classProbabilities(trainingData)
val testPrior = classProbabilities(testData)
val random = trainPriorProbabilities.zip(testPrior).map {
												case(trainProb, testProb) => trainProb*testProb
											}.sum 

println("Accuracy Random Classifier = " + random )
println("Test performance Totale = " + testPer)
sellSignalsButBuy 
sellSignalsOk 
sellSignalButOut
BuySignalsButSell 
BuySignalsButOut 
BuySignalsOk 
outSignalsOk 
val fos = new FileOutputStream("modelli/" + index )
  val oos = new ObjectOutputStream(fos)  
  oos.writeObject(model)  
  oos.close
  index = index +1;
       scala.tools.nsc.io.File("Risultati_USD_JPY_DELETE").appendAll(maxDepth + "," + numTrees + "," + testPer + "," + sellSignalsButBuy + "," +
sellSignalsOk  + "," + sellSignalButOut + "," +
BuySignalsButSell + "," +
BuySignalsButOut + "," +
BuySignalsOk + "," +
outSignalsOk  + "," +
dico_1 + ","+
tot_1 + ","+
dico_0 + ","+
tot_0 + ","+
dico_2 + ","+
tot_2 + ","+
impurity + ","+
cutOff + "," + index +
"\n")
}